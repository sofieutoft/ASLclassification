{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from opencv-python) (1.26.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('CNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "#CONTINUOUS IMAGE\n",
    "def preprocess_image(image):\n",
    "    resized_image = cv2.resize(image, (28, 28))\n",
    "    grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    normalized_image = grayscale_image / 255.0\n",
    "    \n",
    "    # Save the processed grayscale image\n",
    "    cv2.imwrite('processed_frame.jpg', normalized_image * 255)\n",
    "    \n",
    "    reshaped_image = normalized_image.reshape(1, 28, 28, 1)\n",
    "    return reshaped_image\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_AVFOUNDATION)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # Set the frame width to 640 pixels\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # Set the frame height to 480 pixels\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame\")\n",
    "        break\n",
    "    \n",
    "    # Resize the frame to a smaller size\n",
    "    resized_frame = cv2.resize(frame, (320, 240))\n",
    "    \n",
    "    processed_frame = preprocess_image(resized_frame)\n",
    "    \n",
    "    prediction = model.predict(processed_frame)\n",
    "    predicted_letter = chr(prediction.argmax() + 65)\n",
    "    \n",
    "    # Display prediction\n",
    "    cv2.putText(frame, predicted_letter, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('ASL Prediction', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAPTURE IMAGE\n",
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def preprocess_image(image):\n",
    "    resized_image = cv2.resize(image, (28, 28))\n",
    "    grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    normalized_image = grayscale_image / 255.0\n",
    "    reshaped_image = normalized_image.reshape(1, 28, 28, 1)\n",
    "    return reshaped_image\n",
    "\n",
    "def take_photo_and_preprocess(filename='photo.jpg', quality=0.8):\n",
    "    js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "        const div = document.createElement('div');\n",
    "        const capture = document.createElement('button');\n",
    "        capture.textContent = 'Capture';\n",
    "        div.appendChild(capture);\n",
    "        const video = document.createElement('video');\n",
    "        video.style.display = 'block';\n",
    "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "        document.body.appendChild(div);\n",
    "        div.appendChild(video);\n",
    "        video.srcObject = stream;\n",
    "        await video.play();\n",
    "\n",
    "        // Resize the output to fit the video element.\n",
    "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "        // Wait for Capture to be clicked.\n",
    "        await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "        const canvas = document.createElement('canvas');\n",
    "        canvas.width = video.videoWidth;\n",
    "        canvas.height = video.videoHeight;\n",
    "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "        stream.getVideoTracks()[0].stop();\n",
    "        div.remove();\n",
    "        return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "    display(js)\n",
    "    data = eval_js('takePhoto({})'.format(quality))\n",
    "    binary = b64decode(data.split(',')[1])\n",
    "    image = cv2.imdecode(np.frombuffer(binary, np.uint8), -1)\n",
    "    \n",
    "    processed_frame = preprocess_image(image)\n",
    "    \n",
    "    return processed_frame\n",
    "\n",
    "# Capture images and make predictions\n",
    "for i in range(5):  # Capturing 5 images\n",
    "    print(f\"Capturing image {i}...\")\n",
    "    processed_frame = take_photo_and_preprocess()\n",
    "    \n",
    "    prediction = model.predict(processed_frame)\n",
    "    predicted_letter = chr(prediction.argmax() + 65)\n",
    "    print(f\"Predicted letter for image {i}: {predicted_letter}\")\n",
    "    \n",
    "    print(\"Image capture complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "#HAND DETECTION\n",
    "# Load the Haar cascade hand detection model\n",
    "hand_cascade = cv2.CascadeClassifier('hand.xml')\n",
    "\n",
    "def preprocess_image(image):\n",
    "    resized_image = cv2.resize(image, (28, 28))\n",
    "    grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    normalized_image = grayscale_image / 255.0\n",
    "    reshaped_image = normalized_image.reshape(1, 28, 28, 1)\n",
    "    return reshaped_image\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_AVFOUNDATION)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # Set the frame width to 640 pixels\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # Set the frame height to 480 pixels\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect hands in the frame\n",
    "    hands = hand_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Draw rectangles around the detected hands\n",
    "    for (x, y, w, h) in hands:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Adjust the region of interest (ROI) to focus on the hand\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize the ROI to the desired input size for the model\n",
    "        resized_roi = cv2.resize(roi, (28, 28))\n",
    "        \n",
    "        # Preprocess the resized ROI\n",
    "        processed_frame = preprocess_image(resized_roi)\n",
    "        \n",
    "        # Make prediction using the model\n",
    "        prediction = model.predict(processed_frame)\n",
    "        predicted_letter = chr(prediction.argmax() + 65)\n",
    "        \n",
    "        # Display prediction\n",
    "        cv2.putText(frame, predicted_letter, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('ASL Prediction', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ValidatedGraphConfig Initialization failed.\nImageToTensorCalculator: ; RET_CHECK failure (mediapipe/calculators/tensor/image_to_tensor_calculator.cc:144) ValidateOptionOutputDims(options) returned INTERNAL: ; RET_CHECK failure (./mediapipe/calculators/tensor/image_to_tensor_utils.h:136) options.has_output_tensor_float_range() || options.has_output_tensor_int_range() || options.has_output_tensor_uint_range()Output tensor range is required. \nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:64) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:64) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nImageToTensorCalculator: ; RET_CHECK failure (mediapipe/calculators/tensor/image_to_tensor_calculator.cc:144) ValidateOptionOutputDims(options) returned INTERNAL: ; RET_CHECK failure (./mediapipe/calculators/tensor/image_to_tensor_utils.h:136) options.has_output_tensor_float_range() || options.has_output_tensor_int_range() || options.has_output_tensor_uint_range()Output tensor range is required. \nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:64) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:64) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m     11\u001b[0m mphands \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39msolutions\u001b[39m.\u001b[39mhands\n\u001b[0;32m---> 12\u001b[0m hands \u001b[39m=\u001b[39m mphands\u001b[39m.\u001b[39;49mHands()\n\u001b[1;32m     13\u001b[0m mp_drawing \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39msolutions\u001b[39m.\u001b[39mdrawing_utils\n\u001b[1;32m     14\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/mediapipe/python/solutions/hands.py:114\u001b[0m, in \u001b[0;36mHands.__init__\u001b[0;34m(self, static_image_mode, max_num_hands, model_complexity, min_detection_confidence, min_tracking_confidence)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m              static_image_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m              max_num_hands\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     92\u001b[0m              model_complexity\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     93\u001b[0m              min_detection_confidence\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,\n\u001b[1;32m     94\u001b[0m              min_tracking_confidence\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[1;32m     95\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Initializes a MediaPipe Hand object.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m      https://solutions.mediapipe.dev/hands#min_tracking_confidence.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m   \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    115\u001b[0m       binary_graph_path\u001b[39m=\u001b[39;49m_BINARYPB_FILE_PATH,\n\u001b[1;32m    116\u001b[0m       side_inputs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    117\u001b[0m           \u001b[39m'\u001b[39;49m\u001b[39mmodel_complexity\u001b[39;49m\u001b[39m'\u001b[39;49m: model_complexity,\n\u001b[1;32m    118\u001b[0m           \u001b[39m'\u001b[39;49m\u001b[39mnum_hands\u001b[39;49m\u001b[39m'\u001b[39;49m: max_num_hands,\n\u001b[1;32m    119\u001b[0m           \u001b[39m'\u001b[39;49m\u001b[39muse_prev_landmarks\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m static_image_mode,\n\u001b[1;32m    120\u001b[0m       },\n\u001b[1;32m    121\u001b[0m       calculator_params\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    122\u001b[0m           \u001b[39m'\u001b[39;49m\u001b[39mpalmdetectioncpu__TensorsToDetectionsCalculator.min_score_thresh\u001b[39;49m\u001b[39m'\u001b[39;49m:\n\u001b[1;32m    123\u001b[0m               min_detection_confidence,\n\u001b[1;32m    124\u001b[0m           \u001b[39m'\u001b[39;49m\u001b[39mhandlandmarkcpu__ThresholdingCalculator.threshold\u001b[39;49m\u001b[39m'\u001b[39;49m:\n\u001b[1;32m    125\u001b[0m               min_tracking_confidence,\n\u001b[1;32m    126\u001b[0m       },\n\u001b[1;32m    127\u001b[0m       outputs\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    128\u001b[0m           \u001b[39m'\u001b[39;49m\u001b[39mmulti_hand_landmarks\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmulti_hand_world_landmarks\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    129\u001b[0m           \u001b[39m'\u001b[39;49m\u001b[39mmulti_handedness\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m    130\u001b[0m       ])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/mediapipe/python/solution_base.py:248\u001b[0m, in \u001b[0;36mSolutionBase.__init__\u001b[0;34m(self, binary_graph_path, graph_config, calculator_params, graph_options, side_inputs, outputs, stream_type_hints)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m graph_options:\n\u001b[1;32m    245\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_extension(canonical_graph_config_proto\u001b[39m.\u001b[39mgraph_options,\n\u001b[1;32m    246\u001b[0m                       graph_options)\n\u001b[0;32m--> 248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph \u001b[39m=\u001b[39m calculator_graph\u001b[39m.\u001b[39;49mCalculatorGraph(\n\u001b[1;32m    249\u001b[0m     graph_config\u001b[39m=\u001b[39;49mcanonical_graph_config_proto)\n\u001b[1;32m    250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_outputs \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ValidatedGraphConfig Initialization failed.\nImageToTensorCalculator: ; RET_CHECK failure (mediapipe/calculators/tensor/image_to_tensor_calculator.cc:144) ValidateOptionOutputDims(options) returned INTERNAL: ; RET_CHECK failure (./mediapipe/calculators/tensor/image_to_tensor_utils.h:136) options.has_output_tensor_float_range() || options.has_output_tensor_int_range() || options.has_output_tensor_uint_range()Output tensor range is required. \nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:64) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:64) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nImageToTensorCalculator: ; RET_CHECK failure (mediapipe/calculators/tensor/image_to_tensor_calculator.cc:144) ValidateOptionOutputDims(options) returned INTERNAL: ; RET_CHECK failure (./mediapipe/calculators/tensor/image_to_tensor_utils.h:136) options.has_output_tensor_float_range() || options.has_output_tensor_int_range() || options.has_output_tensor_uint_range()Output tensor range is required. \nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:64) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:64) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "_, frame = cap.read()\n",
    "\n",
    "h, w, c = frame.shape\n",
    "\n",
    "img_counter = 0\n",
    "analysisframe = ''\n",
    "letterpred = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        analysisframe = frame\n",
    "        showframe = analysisframe\n",
    "        cv2.imshow(\"Frame\", showframe)\n",
    "        framergbanalysis = cv2.cvtColor(analysisframe, cv2.COLOR_BGR2RGB)\n",
    "        resultanalysis = hands.process(framergbanalysis)\n",
    "        hand_landmarksanalysis = resultanalysis.multi_hand_landmarks\n",
    "        if hand_landmarksanalysis:\n",
    "            for handLMsanalysis in hand_landmarksanalysis:\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                for lmanalysis in handLMsanalysis.landmark:\n",
    "                    x, y = int(lmanalysis.x * w), int(lmanalysis.y * h)\n",
    "                    if x > x_max:\n",
    "                        x_max = x\n",
    "                    if x < x_min:\n",
    "                        x_min = x\n",
    "                    if y > y_max:\n",
    "                        y_max = y\n",
    "                    if y < y_min:\n",
    "                        y_min = y\n",
    "                y_min -= 20\n",
    "                y_max += 20\n",
    "                x_min -= 20\n",
    "                x_max += 20 \n",
    "\n",
    "        analysisframe = cv2.cvtColor(analysisframe, cv2.COLOR_BGR2GRAY)\n",
    "        analysisframe = analysisframe[y_min:y_max, x_min:x_max]\n",
    "        analysisframe = cv2.resize(analysisframe,(28,28))\n",
    "\n",
    "\n",
    "        nlist = []\n",
    "        rows,cols = analysisframe.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                k = analysisframe[i,j]\n",
    "                nlist.append(k)\n",
    "        \n",
    "        datan = pd.DataFrame(nlist).T\n",
    "        colname = []\n",
    "        for val in range(784):\n",
    "            colname.append(val)\n",
    "        datan.columns = colname\n",
    "\n",
    "        pixeldata = datan.values\n",
    "        pixeldata = pixeldata / 255\n",
    "        pixeldata = pixeldata.reshape(-1,28,28,1)\n",
    "        prediction = model.predict(pixeldata)\n",
    "        predarray = np.array(prediction[0])\n",
    "        letter_prediction_dict = {letterpred[i]: predarray[i] for i in range(len(letterpred))}\n",
    "        predarrayordered = sorted(predarray, reverse=True)\n",
    "        high1 = predarrayordered[0]\n",
    "        high2 = predarrayordered[1]\n",
    "        high3 = predarrayordered[2]\n",
    "        for key,value in letter_prediction_dict.items():\n",
    "            if value==high1:\n",
    "                print(\"Predicted Character 1: \", key)\n",
    "                print('Confidence 1: ', 100*value)\n",
    "            elif value==high2:\n",
    "                print(\"Predicted Character 2: \", key)\n",
    "                print('Confidence 2: ', 100*value)\n",
    "            elif value==high3:\n",
    "                print(\"Predicted Character 3: \", key)\n",
    "                print('Confidence 3: ', 100*value)\n",
    "        time.sleep(5)\n",
    "\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "    if hand_landmarks:\n",
    "        for handLMs in hand_landmarks:\n",
    "            x_max = 0\n",
    "            y_max = 0\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "            for lm in handLMs.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "            y_min -= 20\n",
    "            y_max += 20\n",
    "            x_min -= 20\n",
    "            x_max += 20\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
